{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDH5GIJXQ-WU"
   },
   "source": [
    "# Assignment 1-A: Classification\n",
    "\n",
    "The aim of this project is to design a neural network which can take in 21 input attributes from cardiotocography studies of foetal heart rate (FHR) and uterine contraction (UC) and output one of 3 labels based on this data: N(normal), S(suspect), P(pathological).\n",
    "\n",
    "We construct a neural network and experiment with the following hyperparameters to get the fastest yet most accurate model:\n",
    "- Batch size\n",
    "- Hidden neurons\n",
    "- Optimal decay parameter\n",
    "- Number of hidden layers (3 layer network vs. 4 layer network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to5ATheGQ-WV"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We will first import the relevant dependencies and load the data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-aREQb-Q-WV"
   },
   "source": [
    "### Loading dependencies\n",
    "\n",
    "__Tensorflow__ - Constructing the neural networks using the Keras API (Tensorflow 2 is used)  \n",
    "__Scikit-learn__ - Helper functions such as train-test split and cross-validation  \n",
    "__Numpy__ - Data manipulation  \n",
    "__Matplotlib__ - Plotting accuracy and time graphs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0UqfbaXQ-WX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rN-YabRQ-Wc"
   },
   "source": [
    "### Setting up the data\n",
    "\n",
    "1. We first load the data from the csv file, remove the first row which has _nan_, and set the 21 input columns as `X` and the output column as `y`\n",
    "2. We then use the train_test_split function from sklearn to divide the data into a 70:30 ratio for training and testing\n",
    "3. We scale the input values using min-max scaling. This ensures all input values are between 0 and 1, which is beneficial for neural networks since large value ranges can lead to erratic gradient descent which requires more time to converge. With normalized data, we can use a larger learning rate without compromising on time needed for convergence\n",
    "4. Lastly, we convert the `y` values to 0,1,2 to enable easier indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "65Wn4B4bQ-Wd",
    "outputId": "237966d8-fc18-4604-d7c3-92f4e727acc3"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = np.genfromtxt('ctg_data_cleaned.csv', delimiter= ',')\n",
    "data = data[1:] # Remove nan row\n",
    "X, y = data[0:, :21], data[0:,-1].astype(int)\n",
    "\n",
    "print(f\"There are {len(X)} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "eFC3nSZNQ-Wi",
    "outputId": "11ca5d2e-cd8e-4652-e6c7-935222ca7a9e"
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True)\n",
    "\n",
    "print(f\"There are {len(X_train)} rows for training ({100*len(X_train)/len(X)} percent)\")\n",
    "print(f\"There are {len(X_test)} rows for testing ({100*len(X_test)/len(X)} percent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "OA_biFcYQ-Ww",
    "outputId": "e6aaff39-6161-4af1-fcef-f01ab7d384ab"
   },
   "outputs": [],
   "source": [
    "fig1, data_range = plt.subplots()\n",
    "data_range.set_title('Data ranges for input values')\n",
    "data_range.boxplot(X_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1cJ7BSoQ-Wz"
   },
   "source": [
    "As we can see the input values can vary a lot and hence requires normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gS_aUNm_Q-W0",
    "outputId": "93bf20ae-10b0-4818-995e-92a5d39bc4ed"
   },
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"For training data: min={np.min(X_train)}, max={np.max(X_train)}\")\n",
    "print(f\"For testing data: min={np.min(X_test)}, max={np.max(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1mqYRLcQ-W3"
   },
   "outputs": [],
   "source": [
    "# Convert to 0,1,2\n",
    "y_train = y_train-1\n",
    "y_test = y_test-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r__hPDVQ-W6"
   },
   "source": [
    "### Creating the K-folds cross validation function\n",
    "\n",
    "We use 5-fold cross validation in order to find the best model out of 5 after training. The data is split up into 5 sections and 5 different models are trained, each using one of the 5 data splits for validation and using the other 4 for training. This way, data is validated multiple times and ensures that the validation and training set are not biased in any way.\n",
    "\n",
    "We use `StratifiedKFold` from sklearn to ensure that each fold contains equal instances of each output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO9t_Gd8Q-W7"
   },
   "outputs": [],
   "source": [
    "# 5-fold cross-validation function\n",
    "FOLD_COUNT = 5\n",
    "\n",
    "# Setting the hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "OPTIMIZER = SGD(learning_rate=LEARNING_RATE)\n",
    "LOSS_TYPE = SparseCategoricalCrossentropy()\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "def train_model(model, batch_size, epochs):\n",
    "    kf = StratifiedKFold(FOLD_COUNT, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold = 0\n",
    "    fold_accuracies = []\n",
    "    fold_time = []\n",
    "    \n",
    "    for train, test in kf.split(X_train, y_train):\n",
    "        fold += 1\n",
    "        print(f\"Fold #{fold}:\", end=\" \")\n",
    "        \n",
    "        # Splitting the data\n",
    "        fold_X_train = X_train[train]\n",
    "        fold_y_train = y_train[train]\n",
    "        fold_X_test = X_train[test]\n",
    "        fold_y_test = y_train[test]\n",
    "        \n",
    "        # Copy the model\n",
    "        fold_model = tf.keras.models.clone_model(model)\n",
    "        \n",
    "        # Training the model\n",
    "        fold_model.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "\n",
    "        start_time = time.time()\n",
    "        history = fold_model.fit(fold_X_train, \n",
    "                            fold_y_train, \n",
    "                            validation_data=(fold_X_test, fold_y_test),\n",
    "                            batch_size=batch_size, \n",
    "                            epochs=epochs, \n",
    "                            verbose=0)\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        fold_accuracies.append(history.history[\"val_accuracy\"])\n",
    "        fold_time.append(training_time/epochs)\n",
    "\n",
    "        fold_accuracy = history.history[\"val_accuracy\"][epochs-1]\n",
    "\n",
    "        print(f\"Cross validation accuracy: {fold_accuracy}\")\n",
    "\n",
    "    return fold_accuracies, fold_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puTrfWipSHaU"
   },
   "outputs": [],
   "source": [
    "# For getting the average validation accuracies vs. epoch across folds\n",
    "def get_average_accuracy(accuracies):\n",
    "  return np.mean(np.array(accuracies), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pqVB_0xjMxd"
   },
   "outputs": [],
   "source": [
    "# For getting the average time taken to train\n",
    "def get_average_time(times):\n",
    "  return sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQq9i-k1Q-W-"
   },
   "source": [
    "## Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC0tOJa0Q-W-"
   },
   "source": [
    "### 1. Finding the epochs for test loss convergence\n",
    "\n",
    "We design a feedforward neural network which consists of an __input layer__, __1 hidden layer of 10 neurons (ReLU activation function)__, and an __output softmax layer__. \n",
    "\n",
    "- Learning rate alpha = 0.01\n",
    "- L2 regularization (Weight decay beta - 10<sup>-6</sup>) \n",
    "- Batch size - 32\n",
    "\n",
    "We have already used min-max scaling for the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTllwszmQ-W_"
   },
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "model_1 = Sequential([\n",
    "            Dense(units=10, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "govQpsVWQ-XC",
    "outputId": "4e9aadd4-53c0-4d7d-a2b0-d1f1991c5561"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs_1 = 5000\n",
    "batch_size_1 = 32\n",
    "\n",
    "model_1.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "history_1 = model_1.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=batch_size_1, \n",
    "                    epochs=epochs_1, \n",
    "                    verbose=0)\n",
    "print(f\"Model validation accuracy: {max(history_1.history['val_accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "sajEkq8dQ-XF",
    "outputId": "56e616d0-c968-4fb0-eafa-9f70fdc32e4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting train and test set accuracies vs. epoch\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(np.arange(0, epochs_1+1, 200.0))\n",
    "plt.plot(history_1.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history_1.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy (Test Set)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "mLhUZUrKrzT4",
    "outputId": "7483713b-71a2-4888-d4f8-7a148a8f9f8d"
   },
   "outputs": [],
   "source": [
    "# Plotting train and test set loss vs. epoch\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(np.arange(0, epochs_1+1, 200.0))\n",
    "plt.plot(history_1.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history_1.history[\"val_loss\"], label=\"test\")\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Loss (Test Set)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "XOr7yZ-E2J79",
    "outputId": "0e96e77f-9a94-44db-f6d0-5535c68199c9"
   },
   "outputs": [],
   "source": [
    "# Printing the test losses vs. 1000 epochs\n",
    "for epoch in range(0, 5000, 1000):\n",
    "  print(f\"Epoch #{epoch+1000} average test loss : {sum(history_1.history['val_loss'][epoch:epoch+999])/1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA8YQHJ9Q-XN"
   },
   "source": [
    "**Conclusion:** We see the following decreases in losses:  \n",
    "\n",
    "| Epoch | Loss   | Change  |\n",
    "|-------|--------|---------|\n",
    "| 1000  | 0.2692 | -       |\n",
    "| 2000  | 0.2378 | 0.0314  |\n",
    "| 3000  | 0.2392 | -0.0014 |\n",
    "| 4000  | 0.2397 | -0.0005 |\n",
    "| 5000  | 0.2380 | 0.0017  |\n",
    "\n",
    "We can see from the graph that the loss has a sharp decline till the 200th epoch after which the change is loss is miniscule as compared to the training time needed. This is also supported by the accuracy graph where the test accuracy increases sharply till the 200th epoch, gradually increases till 1000 epochs and then either staying the same or decreasing. The average loss for each 1000 epoch interval also shows that the largest change happens in the first 1000 epochs after which the loss change fluctuates around the same value. Since it takes around 200 epochs for the test set loss to converge, we will train the models for around 1000 epochs to account for any variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gh7H56Qa7aaw"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lavDtBrQ-XN"
   },
   "source": [
    "### 2. Finding the optimal batch size\n",
    "\n",
    "Batch size refers to the number of data points fed into the neural network before its weights are updated. Once all the batches representing the data have been fed into the neural network, this consists of an epoch. \n",
    "\n",
    "We will explore the effect of batch size on accuracy and training time using the following batch sizes:  \n",
    "__Search Space__ - [4,8,16,32,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pa9nvZVUQ-XO"
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "batch_sizes = [4,8,16,32,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "r8X9pl9sQ-XT",
    "outputId": "001db34d-b421-4dd2-f2bb-aadea0d18197"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "model_2 = Sequential([\n",
    "            Dense(units=10, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "avg_batch_accuracies = {}\n",
    "avg_time = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\" === Batch Size {batch_size} ===\")\n",
    "    model_2_copy = tf.keras.models.clone_model(model_2)\n",
    "    fold_accuracies, fold_times = train_model(model_2_copy, batch_size, EPOCHS)\n",
    "    \n",
    "    avg_batch_accuracies[batch_size] = get_average_accuracy(fold_accuracies)\n",
    "    avg_time[batch_size] = get_average_time(fold_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "wxyEN529Q-XW",
    "outputId": "39e0fb7a-fb86-4ef7-eb37-dfde25ec8d75"
   },
   "outputs": [],
   "source": [
    "# Plotting average validation accuracies vs. epoch for each batch size\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for batch_size, accuracy in avg_batch_accuracies.items():\n",
    "    plt.plot(accuracy, label=f\"val-{batch_size}\")\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "ZkXm9Pi4Q-Xa",
    "outputId": "cde034d6-6ee0-432c-a8c4-1897fb8cc98f"
   },
   "outputs": [],
   "source": [
    "# Plotting training time for the best model in each batch size\n",
    "plt.xlabel('Batch Size', fontsize=18)\n",
    "plt.ylabel('Time (sec)', fontsize=16)\n",
    "plt.bar(range(len(avg_time)), list(avg_time.values()), align='center');\n",
    "plt.xticks(range(len(avg_time)), list(avg_time.keys()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "zLDVgrrZiLHr",
    "outputId": "0e6ede4d-9e4e-4464-e908-38b6ee110edb"
   },
   "outputs": [],
   "source": [
    "# Plot average validation accuracy for each batch size\n",
    "batch_accuracies = [accuracies[EPOCHS-1] for accuracies in avg_batch_accuracies.values()]\n",
    "\n",
    "print(batch_accuracies)\n",
    "plt.xlabel('Batch Size', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.bar(range(len(avg_batch_accuracies.keys())), batch_accuracies, align='center');\n",
    "plt.xticks(range(len(avg_batch_accuracies.keys())), avg_batch_accuracies.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-kjVknhQ-Xc"
   },
   "source": [
    "__Conclusion:__ We notice the following:\n",
    "\n",
    "| Batch Size | Cross Validation Ranking | Training Speed Ranking |\n",
    "|------------|--------------------------|------------------------|\n",
    "| 4          | 2                        | 5                      |\n",
    "| 8          | 1                        | 4                      |\n",
    "| 16         | 3                        | 3                      |\n",
    "| 32         | 4                        | 2                      |\n",
    "| 64         | 5                        | 1                      |\n",
    "\n",
    "- Larger batch sizes are faster due to lesser overhead of loading a few large batches as opposed to many small batches\n",
    "- Larger batch sizes generally have lower validation accuracies because they converge to sharp minimizers [[1]](https://arxiv.org/pdf/1609.04836.pdf) which vary sharply and it is harder to escape sharp local minima. This is also why they generally have higher losses.\n",
    "- Larger batch sizes take longer to converge to the highest accuracy, which increases training time\n",
    "\n",
    "We see that __batch size of 8__ has the highest average cross-validation accuracy. It also takes half the amount of training time as batch size of 4 and takes very less time before its accuracy begins to converge which can reduce the epochs needed to train. We shall continue with a __batch size of 8__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYLAQx2QQ-Xd"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fulsiqLJQ-Xj",
    "outputId": "e4abff30-e735-4817-e5c2-3afe6dc95d5c"
   },
   "outputs": [],
   "source": [
    "# Training on optimal batch size\n",
    "optimal_model_2 = Sequential([\n",
    "            Dense(units=10, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "\n",
    "\n",
    "optimal_model_2.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "history_2 = optimal_model_2.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=0)\n",
    "print(f\"Testing accuracy: {history_2.history['val_accuracy'][EPOCHS-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "UNuhRJKbe098",
    "outputId": "d92a2356-a5c0-44c5-e7c1-45dd210695ee"
   },
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history_2.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history_2.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkskKgMsQ-Xm"
   },
   "source": [
    "### 3. Finding the optimial number of neurons\n",
    "\n",
    "Neural networks are composed of layers and each layer has a certain number of \"neurons\". These neurons pass messages between each layer and help approximate functions.\n",
    "\n",
    "We will explore the effect of number of neurons on accuracy using the following number of hidden layer neurons:  \n",
    "__Search Space__ - [5,10,15,20,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdRtBegPQ-Xn"
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "model_sizes = [5,10,15,20,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "xoiNRMg9Q-Xp",
    "outputId": "fb0d469c-ae1a-4f90-8485-38bce59eca30"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "avg_model_size_accuracies = {}\n",
    "\n",
    "for model_size in model_sizes:\n",
    "    print(f\" === Neuron Count {model_size} ===\")\n",
    "    model_3 = Sequential([\n",
    "            Dense(units=model_size, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "    fold_accuracies, _ = train_model(model_3, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    avg_model_size_accuracies[model_size] = get_average_accuracy(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "mg8g8BCkQ-Xs",
    "outputId": "d1c63971-667f-4776-c938-9f7802d301e5"
   },
   "outputs": [],
   "source": [
    "# Plotting average validation accuracies vs. epoch for each neuron count\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for model_size, accuracy in avg_model_size_accuracies.items():\n",
    "    plt.plot(accuracy, label=f\"val-{model_size}\")\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "vn79pJ8rJEIG",
    "outputId": "179ed3fe-b194-4c80-d614-7c445b0685d8"
   },
   "outputs": [],
   "source": [
    "# Plot average validation accuracy for each neuron count\n",
    "model_size = [accuracies[EPOCHS-1] for accuracies in avg_model_size_accuracies.values()]\n",
    "\n",
    "print(model_size)\n",
    "plt.xlabel('Neurons', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.bar(range(len(avg_model_size_accuracies.keys())), model_size, align='center');\n",
    "plt.xticks(range(len(avg_model_size_accuracies.keys())), avg_model_size_accuracies.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seMK54nuQ-Xu"
   },
   "source": [
    "__Conclusion:__ From the graphs of training and validation accuracy vs. epoch, we can see that models with more neurons are able to converge faster. This is because the additional neurons are able to learn more complex patterns in the data faster. \n",
    "\n",
    "This is supported by the bar charts where the average accuracy is higher for the larger models.\n",
    "\n",
    "However, we notice that the average validation loss for 25 neurons is higher than that of 20 neurons. This may be because the increased neurons were able to memorise the training set and overfit the data while not being generalizable to the validation set\n",
    "\n",
    "Since the 20 neuron model has one of the highest validation accuracies , we will continue with __20 neurons__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HCdChcyQ-Xw"
   },
   "outputs": [],
   "source": [
    "NEURON_COUNT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "oQF0Wt4tQ-Xy",
    "outputId": "8321362c-ccb6-4a04-a2e9-eefdceec1b54",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training on optimal neuron count and showing train and test accuracy\n",
    "\n",
    "model_3 = Sequential([\n",
    "            Dense(units=NEURON_COUNT, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\", kernel_regularizer=l2(10**(-6)))\n",
    "        ])\n",
    "model_3.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "history_3 = model_3.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=0)\n",
    "\n",
    "print(f\"Testing accuracy: {history_3.history['val_accuracy'][EPOCHS-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "L3FP8sjjiYYJ",
    "outputId": "7a7b6cff-2aec-4c34-cd8d-8a728fff76d0"
   },
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history_3.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history_3.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3IrWNqKQ-X4"
   },
   "source": [
    "### 4. Finding the optimal decay parameter\n",
    "\n",
    "Regularization is a technique used to stop overfitting of data. The regularization term is the sum of squares of the feature weights. When added to the cost, this stops the feature weights from exploding since larger weights mean larger cost. The decay parameter decides how much the regularization term affects the cost.\n",
    "\n",
    "![L2 regularization](https://www.oreilly.com/library/view/hands-on-machine-learning/9781788393485/assets/320843d0-3683-4422-80b2-c2913f8d02d4.png)\n",
    "\n",
    "We will explore the effect of decay parameter on accuracy using the following decay parameters:  \n",
    "__Search space__ - [0, 10<sup>-3</sup>, 10<sup>-6</sup>, 10<sup>-9</sup>, 10<sup>-12</sup>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64wIBuDJQ-X4"
   },
   "outputs": [],
   "source": [
    "#Define the search space\n",
    "decay_vals = [0,10**(-3),10**(-6),10**(-9),10**(-12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "oOvSWSC3Q-X8",
    "outputId": "9d04e4d0-f0c7-4a8f-c960-416567eeae27"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "avg_decay_val_accuracies = {}\n",
    "\n",
    "for decay_val in decay_vals:\n",
    "    print(f\" === Decay Parameter {decay_val} ===\")\n",
    "    model_4 = Sequential([\n",
    "            Dense(units=NEURON_COUNT, activation=\"relu\", kernel_regularizer=l2(decay_val)),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "    fold_accuracies, _ = train_model(model_4, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    avg_decay_val_accuracies[decay_val] = get_average_accuracy(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "G3V2GQ_MQ-X_",
    "outputId": "2030d5fe-2ea4-4b78-c314-b6463f881e51"
   },
   "outputs": [],
   "source": [
    "# Plotting average validation accuracies vs. epoch for each decay parameter\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for decay_val, accuracy in avg_decay_val_accuracies.items():\n",
    "    plt.plot(accuracy, label=f\"val-{decay_val}\")\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "2nH9h8L0Pmp0",
    "outputId": "4d63adb5-44ee-4b2c-9469-d0a0b2110ede"
   },
   "outputs": [],
   "source": [
    "# Plot average validation accuracy for each decay parameter\n",
    "decay_val = [accuracies[EPOCHS-1] for accuracies in avg_decay_val_accuracies.values()]\n",
    "\n",
    "print(decay_val)\n",
    "plt.xlabel('Decay Parameter', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.bar(range(len(avg_decay_val_accuracies.keys())), decay_val, align='center');\n",
    "plt.xticks(range(len(avg_decay_val_accuracies.keys())), avg_decay_val_accuracies.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LmjDnUaQ-YB"
   },
   "source": [
    "__Conclusion:__ We see from the bar charts that __10<sup>-9</sup>__ has the highest average validation accuracy. The graph also shows us that it has above average cross validation accuracy after epoch 500. Since the 10<sup>-9</sup> decay model has the highest cross-validation accuracy, we shall continue with a __decay parameter of 10<sup>-9</sup>__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-hdvf9HQ-YC"
   },
   "outputs": [],
   "source": [
    "DECAY = 10**(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "W9iyvdn5Q-YE",
    "outputId": "a68eb6aa-b116-44bf-fb98-e9b4e89d0989"
   },
   "outputs": [],
   "source": [
    "# Training on optimal decay parameter and showing train and test accuracy\n",
    "\n",
    "model_4 = Sequential([\n",
    "            Dense(units=NEURON_COUNT, activation=\"relu\", kernel_regularizer=l2(DECAY)),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "model_4.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "history_4 = model_4.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=0)\n",
    "print(f\"Testing accuracy: {history_4.history['val_accuracy'][EPOCHS-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "YIee1ktCH8hc",
    "outputId": "279c8e29-fa5b-41ce-e396-1723b8a0105b"
   },
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history_4.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history_4.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FajmNStCQ-YG"
   },
   "source": [
    "## 5. Training a 4-layer neural network\n",
    "\n",
    "Adding more layers to a neural network introduces more neurons which can compute more complex functions.\n",
    "\n",
    "We will explore the effect of layer count on accuracy by comparing the previous 3 layer model and a new 4 layer model with the following hyperparameters:\n",
    "\n",
    "- 10 neurons in each hidden layer\n",
    "- Batch size of 32\n",
    "- Decay parameter of 10<sup>-6</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pi4EhnyrQ-YG",
    "outputId": "31efcdee-3cd1-4d5f-9815-171fceaef55e"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "model_5 = Sequential([\n",
    "            Dense(units=10, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=10, activation=\"relu\", kernel_regularizer=l2(10**(-6))),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "model_5.compile(optimizer=OPTIMIZER, loss=LOSS_TYPE, metrics=METRICS)\n",
    "history_5 = model_5.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=0)\n",
    "print(f\"Testing accuracy: {history_5.history['val_accuracy'][EPOCHS-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "CK5BC6AtQ-YK",
    "outputId": "5c1c4e3a-b9c8-40a1-d308-ee881e6dea31"
   },
   "outputs": [],
   "source": [
    "# Plotting train and test accuracy\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history_5.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history_5.history[\"val_accuracy\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igu3OGpRQ-YM"
   },
   "source": [
    "__Conclusion:__ We see that the test set accuracy of the 4-layer network is 0.918 which is lower than the 0.920 accuracy of the optimal 3-layer network. We also see that the 4-layer model converges to its highest accuracy faster than the 3 layer-model which continues to slightly increase its test set accuracy even after 1000 epochs. This is because the additional neurons are able to learn more features from the dataset faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siG7BhgcdenB"
   },
   "source": [
    "## Extra: Comparing learning rates\n",
    "\n",
    "The learning rate defines the magnitude in change of the weights based on the loss of the output. We will compare train models of learning rates of 1, 0.1, and 0.0001 and compare the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0ddiUBrdrvc"
   },
   "outputs": [],
   "source": [
    "#Define the search space\n",
    "learning_rates = [0.0001, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "nTFY421WfAkt",
    "outputId": "a3b9c0ef-da43-404c-8242-4f779de2d8ae"
   },
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "avg_learning_rate_accuracies = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\" === Learning Rate {learning_rate} ===\")\n",
    "    OPTIMIZER = SGD(learning_rate=learning_rate)\n",
    "    model_6 = Sequential([\n",
    "            Dense(units=NEURON_COUNT, activation=\"relu\", kernel_regularizer=l2(DECAY)),\n",
    "            Dense(units=3, activation=\"softmax\")\n",
    "        ])\n",
    "    fold_accuracies, _ = train_model(model_6, BATCH_SIZE, 500)\n",
    "    \n",
    "    avg_learning_rate_accuracies[learning_rate] = get_average_accuracy(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "Zqg1LwoTfZJn",
    "outputId": "0f3b4685-e232-468f-fbf1-1d2afab29061"
   },
   "outputs": [],
   "source": [
    "# Plotting average validation accuracies vs. epoch for each learning rate\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for learning_rate, accuracy in avg_learning_rate_accuracies.items():\n",
    "    plt.plot(accuracy, label=f\"val-{learning_rate}\")\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "pm-rS32sfiey",
    "outputId": "dd105880-91ab-4700-87ea-3d7d93baac76"
   },
   "outputs": [],
   "source": [
    "# Plot average validation accuracy for each learning rate\n",
    "learning_rate = [accuracies[500-1] for accuracies in avg_learning_rate_accuracies.values()]\n",
    "\n",
    "print(learning_rate)\n",
    "plt.xlabel('Learning Rate', fontsize=18)\n",
    "plt.ylabel('Accuracy (Cross Validation)', fontsize=16)\n",
    "plt.bar(range(len(avg_learning_rate_accuracies.keys())), learning_rate, align='center');\n",
    "plt.xticks(range(len(avg_learning_rate_accuracies.keys())), avg_learning_rate_accuracies.keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRkTuExUfrhY"
   },
   "source": [
    "We can see that the model with learning rate `0.0001` has the lowest cross-validation accuracy. This is because the slow change in weights does not lead to an improvement in accuracy over time. \n",
    "\n",
    "The model with learning rate `1` is also not able to achieve a high accuracy. This is possibly because the large learning rate overshoots the weights needed for the minimum loss, making the model osciallate around a lower accuracy.\n",
    "\n",
    "The model with learning rate of `0.1` which is in the middle of the other two seems to have the perfect balance and is able to attain the highest accuracy as the change in weights are not too less and not too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwJ5SosNvwhJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "to5ATheGQ-WV",
    "qC0tOJa0Q-W-",
    "_lavDtBrQ-XN",
    "QkskKgMsQ-Xm",
    "B3IrWNqKQ-X4",
    "FajmNStCQ-YG",
    "siG7BhgcdenB"
   ],
   "name": "1A.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
